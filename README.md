# MSDS692_Practicum
AI is increasingly intertwined in our daily lives. Even though the integration of AI into our daily lives has several benefits, it also poses a threat to our survival. Identifying whether AI or a human generates the text can allow us to identify fake news, facilitate plagiarism identification, and promote security and privacy. Recognizing the potential threat of AI-generated texts, this project aims to develop a classification model capable of detecting whether text is generated by AI or a human. Additionally, the project seeks to identify common patterns inherent in AI-generated and human-generated texts.

In this project, I am utilizing three datasets obtained from Kaggle and Hugging Face. These datasets are employed to train classification models and identify common patterns. The classification models employed in this project include logistic regression, random forest, and naive Bayes models. The model that produces the highest accuracy will be selected, and testing will be performed using the test data. In addition to developing the model, this project incorporates n-gram analysis to identify patterns and information in the text data. Tools such as Jupiter Notebook in Python 3 will be utilized for interacting with and analyzing the data. Data cleansing will be conducted using appropriate methods, and visualization will be performed to showcase the most frequent words in both AI and human-generated texts. Tokenization of the text will be implemented to break it into tokens (individual words), and lemmatization will be carried out to transform words into their basic root form, reducing dimensionality for improved machine learning model learning. Furthermore, TF-IDF vectorization will be applied to convert the text into numerical vectors suitable for machine-learning tasks.
