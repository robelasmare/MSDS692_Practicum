# MSDS692_Practicum
## AI vs Human-Generated Text Classification
### Project Objectives and Problem Definition
AI is increasingly intertwined in our daily lives. Even though the integration of AI into our daily lives has several benefits, it has the potential to pose a threat to our survival. Identifying whether AI or a human generates the text can allow us to identify fake news, facilitate plagiarism identification, and promote security and privacy. Recognizing the potential threat of AI-generated texts, this project aims to develop a classification model with the best accuracy capable of detecting whether AI or a human generates text. Additionally, the project seeks to identify common patterns inherent in AI-generated and human-generated texts through exploratory data analysis (EDA).
### Project Details
In this project, three datasets obtained from Kaggle and Hugging Face are utilized. The idea is to combine the three datasets to make a big dataset. Before the unification of the dataset, appropriate exploratory data analysis (EDA) is performed on each dataset. After concatenation of the data for computational efficiency and easy visualization, only 400,000 rows of data are selected from over 7,000,000 rows. Data processing using regular expression (regex) Python module is implemented to preprocess text data. Visualization using bar plots and word clouds is implemented to visualize the frequency of words in AI-generated and human-generated tokenized text. Transforming the data into lowercase and removing stopwords before the visualization is performed to showcase the most important frequent words in both AI and human-generated texts. Tokenization of the text is implemented to break it into tokens (individual words), and lemmatization is carried out to transform words into their basic root form, reducing dimensionality for improved machine learning model learning. Furthermore, TF-IDF vectorization is applied to convert the text into numerical vectors suitable for machine-learning tasks.
The classification models employed in this project include logistic regression, naive Bayes, random forest, and other models. The model that produces the highest accuracy is selected and tested using some texts that are generated by AI and human. Tools such as Jupyter Notebook in Python 3 are utilized for interacting with and analyzing the data.
